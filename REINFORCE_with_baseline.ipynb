{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "REINFORCE_with_baseline.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+ejBL8E9z9NzLof4SVeGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/linjunzh/reinforcement_learning/blob/master/REINFORCE_with_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlUInTAmXc4v",
        "colab_type": "text"
      },
      "source": [
        "This code is more like the REINFORCE with baseline, which depends on episode samples and the computed returns for each episode.\n",
        "\n",
        "In the following code, critic is indeed the baseline that approximates the state value V(s)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkZeIzYRf3B1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.losses import Huber\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21Z6AkrmghzN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7069fb0d-8a03-4c37-8d59-46fba4d64391"
      },
      "source": [
        "''' Environment '''\n",
        "\n",
        "seed = 42\n",
        "gamma = 0.99    # discount factor\n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.seed(seed)\n",
        "eps = np.finfo(np.float32).eps.item()   # smallest number s.t. 1.0+eps != 1.0\n",
        "\n",
        "nb_states = env.observation_space.shape[0]\n",
        "nb_actions = env.action_space.n\n",
        "\n",
        "print('nb_states=%d, nb_actions=%d' % (nb_states, nb_actions))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb_states=4, nb_actions=2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om3-3j2LiAPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' Actor-critic network '''\n",
        "\n",
        "num_hidden = 128\n",
        "\n",
        "input = Input(shape=(nb_states,))\n",
        "x = Dense(num_hidden, activation='relu')(input)\n",
        "action = Dense(nb_actions, activation='softmax')(x)\n",
        "critic = Dense(1)(x)\n",
        "\n",
        "model = Model(inputs=input, outputs=[action, critic])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6g2YqBTi9aG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d00bbf96-1bf6-467e-a047-839e57c6223e"
      },
      "source": [
        "''' Train '''\n",
        "\n",
        "optimizer = Adam(learning_rate=0.01)\n",
        "huber_loss = Huber()\n",
        "\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "while True:   # run until solved\n",
        "  state = env.reset()\n",
        "  episode_reward = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "      state = tf.convert_to_tensor(state)\n",
        "      state = tf.expand_dims(state, 0)\n",
        "\n",
        "      # predict action probabilities and estimate future rewards from environment state\n",
        "      action_probs, critic_value = model(state)\n",
        "      critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "      # sample action\n",
        "      action = np.random.choice(nb_actions, p=np.squeeze(action_probs))\n",
        "      action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "      # apply the sampled action\n",
        "      state, reward, done, _ = env.step(action)\n",
        "      rewards_history.append(reward)\n",
        "      episode_reward += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    # update running reward to check condition for solving\n",
        "    running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "    # calculate expected value from rewards\n",
        "    returns = []\n",
        "    discounted_sum = 0\n",
        "    for r in rewards_history[::-1]:\n",
        "      discounted_sum = r + gamma * discounted_sum\n",
        "      returns.insert(0, discounted_sum)\n",
        "\n",
        "    # normalize returns\n",
        "    returns = np.array(returns)\n",
        "    returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "    returns = returns.tolist()\n",
        "\n",
        "    # calculate loss values to update network\n",
        "    history = zip(action_probs_history, critic_value_history, returns)\n",
        "    actor_losses, critic_losses = [], []\n",
        "    for log_prob, value, ret in history:\n",
        "      diff = ret - value\n",
        "      actor_losses.append(-log_prob * diff)   # actor loss\n",
        "\n",
        "      # the critic must be updated so that it predicts a better estimate of\n",
        "      # the future rewards\n",
        "      critic_losses.append(huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0)))\n",
        "\n",
        "    # backpropagation\n",
        "    loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    # clear the loss and reward history\n",
        "    action_probs_history.clear()\n",
        "    critic_value_history.clear()\n",
        "    rewards_history.clear()\n",
        "\n",
        "  # Log details\n",
        "  episode_count += 1\n",
        "  if episode_count % 10 == 0:\n",
        "    template = 'running reward: {:.2f} at episode {}'\n",
        "    print(template.format(running_reward, episode_count))\n",
        "\n",
        "  if running_reward > 495:  # condition to consider the task solved\n",
        "    print('Solved at episode {}!'.format(episode_count))\n",
        "    break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running reward: 12.88 at episode 10\n",
            "running reward: 19.26 at episode 20\n",
            "running reward: 21.33 at episode 30\n",
            "running reward: 22.27 at episode 40\n",
            "running reward: 37.67 at episode 50\n",
            "running reward: 52.37 at episode 60\n",
            "running reward: 43.74 at episode 70\n",
            "running reward: 43.73 at episode 80\n",
            "running reward: 72.29 at episode 90\n",
            "running reward: 82.41 at episode 100\n",
            "running reward: 169.29 at episode 110\n",
            "running reward: 248.35 at episode 120\n",
            "running reward: 271.23 at episode 130\n",
            "running reward: 274.06 at episode 140\n",
            "running reward: 280.45 at episode 150\n",
            "running reward: 281.87 at episode 160\n",
            "running reward: 240.50 at episode 170\n",
            "running reward: 226.25 at episode 180\n",
            "running reward: 276.56 at episode 190\n",
            "running reward: 284.49 at episode 200\n",
            "running reward: 266.11 at episode 210\n",
            "running reward: 275.18 at episode 220\n",
            "running reward: 345.58 at episode 230\n",
            "running reward: 351.65 at episode 240\n",
            "running reward: 319.74 at episode 250\n",
            "running reward: 274.91 at episode 260\n",
            "running reward: 296.38 at episode 270\n",
            "running reward: 313.06 at episode 280\n",
            "running reward: 342.58 at episode 290\n",
            "running reward: 334.57 at episode 300\n",
            "running reward: 304.71 at episode 310\n",
            "running reward: 332.35 at episode 320\n",
            "running reward: 393.79 at episode 330\n",
            "running reward: 420.74 at episode 340\n",
            "running reward: 307.89 at episode 350\n",
            "running reward: 279.39 at episode 360\n",
            "running reward: 306.97 at episode 370\n",
            "running reward: 384.43 at episode 380\n",
            "running reward: 430.80 at episode 390\n",
            "running reward: 458.57 at episode 400\n",
            "running reward: 471.18 at episode 410\n",
            "running reward: 442.80 at episode 420\n",
            "running reward: 412.87 at episode 430\n",
            "running reward: 418.62 at episode 440\n",
            "running reward: 380.56 at episode 450\n",
            "running reward: 334.13 at episode 460\n",
            "running reward: 304.23 at episode 470\n",
            "running reward: 359.32 at episode 480\n",
            "running reward: 415.77 at episode 490\n",
            "running reward: 398.34 at episode 500\n",
            "running reward: 412.27 at episode 510\n",
            "running reward: 418.42 at episode 520\n",
            "running reward: 387.08 at episode 530\n",
            "running reward: 415.27 at episode 540\n",
            "running reward: 449.27 at episode 550\n",
            "running reward: 450.63 at episode 560\n",
            "running reward: 424.84 at episode 570\n",
            "running reward: 400.92 at episode 580\n",
            "running reward: 397.50 at episode 590\n",
            "running reward: 282.22 at episode 600\n",
            "running reward: 204.15 at episode 610\n",
            "running reward: 197.94 at episode 620\n",
            "running reward: 297.14 at episode 630\n",
            "running reward: 296.35 at episode 640\n",
            "running reward: 358.29 at episode 650\n",
            "running reward: 371.97 at episode 660\n",
            "running reward: 423.35 at episode 670\n",
            "running reward: 454.10 at episode 680\n",
            "running reward: 448.82 at episode 690\n",
            "running reward: 469.36 at episode 700\n",
            "running reward: 472.44 at episode 710\n",
            "running reward: 483.50 at episode 720\n",
            "running reward: 490.12 at episode 730\n",
            "running reward: 494.08 at episode 740\n",
            "Solved at episode 744!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEY9VUwowIHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a744bad9-b14d-492d-865d-d10d5454136e"
      },
      "source": [
        "max_steps_per_episode"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZdNFhhBwQew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}